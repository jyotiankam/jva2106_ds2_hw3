---
author: "Jyoti Ankam"
title: "hw_3"
date: "April 4, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(message = F)
```

Loading the library ISLR

```{r}

library(ISLR)
library(pROC)
library(caret)
library(MASS)
library(class)

```

Viewing the Weekly dataset

```{r}
data(Weekly)
Weekly = Weekly[,-8]
names(Weekly)
View(Weekly)

```
a) SUMMARIES OF THE DATASET:

Let's use the summary() fuction to see the numerical summaries of the dataset

```{r}

summary(Weekly)

```

Let's see some graphical summaries


```{r}
featurePlot(x = Weekly[, 1:7],
            y = Weekly$Direction,
            scales = list(x = list(relation = "free"),
            y = list(relation = "free")),
            plot = "density", pch ="|",
            auto.key = list(columns = 2))
```

```{r}

cor(Weekly[,-8])
plot(Weekly$Volume)
pairs(Weekly)

```

#Summaries

As evident, all the Lags (1 to 5) have similar means and medians which implies that the return percentage has no correlation with time. The correlations between the “lag” variables and today’s returns are close to zero. The only substantial correlation is between “Year” and “Volume”. When we plot “Volume”, we see that it is increasing over time. Apart of these two variables, no other variables display any kind of relationship. 

b) Using the full data set to perform a logistic regression with Direction as the response and the five Lag variables plus Volume as predictors:

# Logistic Regression

```{r}

log.reg <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, family=binomial, data= Weekly)
summary(log.reg)

```

As seen in the results, only Lag2 is statistically significant with a p-value of 0.0296 which is < 0.05
All the other predictors are statisitcally insignificant. Higher p-values indicates that there is not enough evidence to reject Null hypothesis. With p-value of 0.0296, Lag2 displays some statistical significance.

c) Computing the confusion matrix and overall fraction of correct predictions. Briefly explaining what the confusion matrix is telling you:

# Confusion Matrix

We first consider the Bayes classifier (cutoff 0.5) and evaluate its performance on the test data.

```{r}

log.probs <- predict(log.reg,type="response")
log.probs[1:10]

log.pred <- ifelse(log.probs > 0.5, "Up", "Down")

log.table <- table(log.pred,Weekly$Direction)
log.table

accuracy <- (log.table["Down", "Down"] + log.table["Up", "Up"])/nrow(Weekly)
error_rate <- 1 - accuracy

View(accuracy) #0.5610
View(error_rate)#0.4389

sensitivity <-log.table ["Up", "Up"]/(log.table["Down", "Up"] + log.table["Up", "Up"])
View(sensitivity) #0.9206

specificity <-log.table ["Down", "Down"]/(log.table["Down", "Down"] + log.table["Up", "Down"])
View(specificity)#0.1115

```

Based on the accuracy,  the % of correct predictions is 56.1% without making Type 1 or 2 errors (i.e. the percentage of correct predictions on the training data is (54+557)/1089 wich is equal to 56.1065%). This means that we were able to perdict the correct trend 56.1% times and also indicates that our prediction is wrong 43.9 % times (i.e. in other words 43.8934803% is the training error rate, which is often overly optimistic)
In this case, the model predicts well for UP rather than Down. When the market goes Up, the model predicts it right 92.06% of the times, while, when the market goes Down, it predicts it right only 11.15% of the times. That is to say that for weeks when the market goes up, the model is right 92.0661% of the time (557/(48+557)). For weeks when the market goes down, the model is right only 11.1570% of the time (54/(54+430)).

A confusion matrix is a table that is often used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known. Table () is used to create a confusion matrix. The number at the diagonal gives us the number of correct predictions and the number off the diagonal gives us the number of incorrect predictions.

(d) Plotting the ROC curve using the predicted probability from logistic regression and reporting the AUC:

```{r}

roc.glm <- roc(Weekly$Direction, log.probs)
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)

```

The AUC is 0.554

e) Now fitting the logistic regression model using a training data period from 1990 to 2008, with Lag1 and Lag2 as the predictors.

```{r}

training <- (Weekly$Year < 2009)

Weekly.heldout <- Weekly[!training, ]
Direction.heldout <- Weekly$Direction[!training]

log.reg2 <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial, subset = training)
summary(log.reg2)

```
Creating a confusion matrix and performance measures for this heldout model (Note: since the question doen not ask for this, this section is not necessarily a part of the answer to question e)

```{r}
log.probs2 <- predict(log.reg2, newdata=Weekly.heldout, type="response")
log.pred2 <- ifelse(log.probs2 > 0.5, "Up", "Down")
log.table2 <- table(log.pred2, Direction.heldout)

accuracy2 <- (log.table["Down", "Down"] + log.table["Up", "Up"])/nrow(Weekly.heldout)
error_rate2 <- 1 - accuracy2
sensitivity2 <-log.table2 ["Up", "Up"]/(log.table2["Down", "Up"] + log.table2["Up", "Up"])
specificity2 <-log.table2 ["Down", "Down"]/(log.table2["Down", "Down"] + log.table2["Up", "Down"])


accuracy2
error_rate2
sensitivity2
specificity2
```

Plotting the ROC curve using the held out data (that is, the data from 2009 and 2010) and reporting the AUC:

```{r}

roc.glm2 <- roc(Direction.heldout, log.probs2)
plot(roc.glm2, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm2), col = 4, add = TRUE)

```
The AUC is 0.556

f) Repeating (e) using LDA which implies Linear Discriminant Analysis and QDA which implied Quadratic Discriminant Analysis:

```{r}

fit.lda <- lda(Direction ~ Lag1 + Lag2, data = Weekly, subset = training)
fit.lda
plot(fit.lda)

```
```{r}

pred.lda <- predict(fit.lda, Weekly.heldout)
log.table3 <- table(pred.lda$class, Direction.heldout)
log.table3

accuracy3 <- (log.table3["Down", "Down"] + log.table3["Up", "Up"])/nrow(Weekly.heldout)
error_rate3 <- 1 - accuracy3
sensitivity3 <-log.table3 ["Up", "Up"]/(log.table3["Down", "Up"] + log.table3["Up", "Up"])
specificity3 <-log.table3 ["Down", "Down"]/(log.table3["Down", "Down"] + log.table3["Up", "Down"])

```

```{r}

lda.probs = pred.lda$posterior[,2]
roc.glm3 <- roc(as.numeric(Direction.heldout), as.numeric(lda.probs))
plot(roc.glm3, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm3), col = 4, add = TRUE)

```
The AUC for the logistic regression model with Lag1 and Lag2 is 0.557

```{r}

fit.qda <- qda(Direction ~ Lag1 + Lag2, data = Weekly, subset = training)
fit.qda

```

```{r}

pred.qda <- predict(fit.qda, Weekly.heldout)
log.table4 <- table(pred.qda$class, Direction.heldout)
log.table4

accuracy4 <- (log.table4["Down", "Down"] + log.table4["Up", "Up"])/nrow(Weekly.heldout)
error_rate4 <- 1 - accuracy4
sensitivity4 <-log.table4 ["Up", "Up"]/(log.table4["Down", "Up"] + log.table4["Up", "Up"])
specificity4 <-log.table4 ["Down", "Down"]/(log.table4["Down", "Down"] + log.table4["Up", "Down"])
```


```{r}

qda.probs = pred.qda$posterior[,2]
roc.glm4 <- roc(as.numeric(Direction.heldout), as.numeric(qda.probs))
plot(roc.glm4, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm4), col = 4, add = TRUE)


```
The AUC is 0.529

(g) Repeat (e) using KNN. Briefly discuss your results.

```{r}

ctrl <- trainControl(method = "repeatedcv",
                      repeats = 5,
                      summaryFunction = twoClassSummary,
                      classProbs = TRUE)
set.seed(1)
model.knn <- train(x= Weekly[training, 1:2], 
                   y = Weekly$Direction[training],
                   method = "knn",
                   preProcess = c("center", "scale"),
                   tuneGrid = data.frame(k = seq(1, 50, by = 2)),
                   trControl = ctrl,
                   metric = 'ROC')


ggplot(model.knn)                   

```

```{r}

pred.knn <- predict(model.knn, newdata = Weekly.heldout, type = "prob")[,2]

roc.knn <- roc(Direction.heldout, pred.knn)
plot(roc.knn, legacy.axes = TRUE, print.auc = TRUE)
```

The AUC for the K Nearest Neighbor is 0.604

Looking at the models, it appears that logistic regression model works a little better than just random guessing. But since we have trained and tested the model, on the same set of observations, our results may be incorrect and misleading. Our training error rate which is 43.893% seems to be too optimistic and underestimating the test error rate.
Therefore, for better prediction and accuracy of the logistic regression model, we partition the data and fit the model and then estimate how well it predicts the heldout data, thus getting a more realistic error rate.

In order to compare the performance of a classifier based on different cut off points, we  get the AUC of the heldout data. More or less, the AUC of the helout data is similar to the full dataset.
We get the AUC of the logisitc model with the complete data set as 0.557 and the AUC of the heldout data as 0.554. The AUC for the K Nearest Neighbor is 0.604. The AUC for QDA is 0.529 and for LDA is 0.557. Although, AUC for KNN is slightly higher, overall they all perform similarly as it is hard to predict changes in stock price based on the previous days (lag1 and lag2). Additionally, using the lag1 and lag2 predictors, the logisitic regression p-values are not significant for the data subset suggesting that lag1 and lag2 may not be significantly associated with the Direction. Predictors which aren't associated with the outcome cause an increase in variance without any corresponding decrease in bias, thereby resulting in inaccurate/inadequate predictions.

